{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 需要修改必要的文件路径！！！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 安装paddlenlp最新版本\n",
    "# !pip install --upgrade paddlenlp -i https://pypi.org/simple\n",
    "\n",
    "%cd multi-skill_dialogue/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 注意：脚本默认只取每个数据集的部分语料进行处理作为基线模型的训练数据，需根据需求自行修改数据处理策略\n",
    "%ls\n",
    "python ./tools/convert_data_to_numerical.py ./tools/spm.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade paddlenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddle.io import DataLoader\n",
    "from data import DialogueDataset\n",
    "\n",
    "# 训练batch_size\n",
    "batch_size = 8192\n",
    "# 组batch进行排序和shuffle的pool_size\n",
    "sort_pool_size = 65536\n",
    "\n",
    "# 训练集路径，注意与数据预处理输出路径保持一致\n",
    "train_data_path = './datasets/train.txt' \n",
    "# 初始化Dataset\n",
    "train_dataset = DialogueDataset(\n",
    "        train_data_path,\n",
    "        batch_size,\n",
    "        tokenizer.pad_token_id,\n",
    "        tokenizer.cls_token_id,\n",
    "        sort_pool_size,\n",
    "        mode='train')\n",
    "# 初始化Dataloader\n",
    "train_dataloader = DataLoader(train_dataset, return_list=True, batch_size=None)\n",
    "\n",
    "# 开发集路径，注意与数据预处理输出路径保持一致\n",
    "valid_data_path = './datasets/valid.txt' \n",
    "valid_dataset = DialogueDataset(\n",
    "    valid_data_path,\n",
    "    batch_size,\n",
    "    tokenizer.pad_token_id,\n",
    "    tokenizer.cls_token_id,\n",
    "    sort_pool_size,\n",
    "    mode='valid')\n",
    "valid_dataloader = DataLoader(valid_dataset, return_list=True, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测batch_size\n",
    "batch_size = 4\n",
    "\n",
    "# 测试集路径，注意与数据预处理输出路径保持一致\n",
    "test_data_path = './datasets/test.txt' \n",
    "test_dataset = DialogueDataset(\n",
    "    test_data_path,\n",
    "    batch_size,\n",
    "    tokenizer.pad_token_id,\n",
    "    tokenizer.cls_token_id,\n",
    "    mode='test')\n",
    "test_dataloader = DataLoader(test_dataset, return_list=True, batch_size=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、接下来运行脚本train.sh来利用unified_transformer-12L-cn训练，一开始考虑使用ernie-3.0-medium-zh，但最终还是选择使用unified_transformer-12L-cn；优化器选用AdamW，注意修改路径和必要参数！！！\n",
    "\n",
    "2、在脚本中设置的 CUDA_VISIBLE_DEVICES可根据服务器情况来选择性添加，整体模型参数与流程与baseline一致\n",
    "\n",
    "3、每1000步并且loss下降或间隔100000步就保存模型参数并进行评测，从而增多保存模型参数，便于调试\n",
    "\n",
    "4、之后运行predict.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('wrp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f077947b6bd2905a281819c35d5f22b130ca96deced6ffc9990d95dc6897cd45"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
